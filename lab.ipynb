{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2018 09 07 Lecture\n",
    "\n",
    "@author: nhjeong & wilee\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Activation, Conv2D, Input, MaxPool2D, UpSampling2D, Concatenate, BatchNormalization, Add\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 44000\n",
    "\n",
    "X_mat = h5py.File('database/db.mat')\n",
    "X_input = X_mat['db']\n",
    "X_train = X_input[0:N, ]\n",
    "X_val = X_input[N:, ]\n",
    "\n",
    "Y_mat = h5py.File('database/gt.mat')\n",
    "Y_input = Y_mat['gt']\n",
    "Y_train = Y_input[0:N, ]\n",
    "Y_val = Y_input[N:, ]\n",
    "\n",
    "X_mat_test = h5py.File('database/test_db.mat')\n",
    "X_input_test = X_mat_test['test_db']\n",
    "X_test = X_input_test[0:, ]\n",
    "\n",
    "Y_mat_test = h5py.File('database/test_gt.mat')\n",
    "Y_input_test = Y_mat_test['test_gt']\n",
    "Y_test = Y_input_test[0:, ]\n",
    "\n",
    "\n",
    "X_mat.close()\n",
    "Y_mat.close()\n",
    "X_mat_test.close()\n",
    "Y_mat_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define the two different types of layers that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down(input_layer, filters, pool=True):\n",
    "    conv = Conv2D(filters, (3, 3), padding='same')(input_layer)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "\n",
    "    residual = Conv2D(filters, (3, 3), padding='same')(conv)\n",
    "    residual = BatchNormalization()(residual)\n",
    "    residual = Activation('relu')(residual)\n",
    "\n",
    "    if pool:\n",
    "        max_pool = MaxPool2D()(residual)\n",
    "        return max_pool, residual\n",
    "    else:\n",
    "        return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up(input_layer, residual, filters):\n",
    "    filters = int(filters)\n",
    "    upsample = UpSampling2D()(input_layer)\n",
    "    upconv = Conv2D(filters, kernel_size=(2, 2), padding=\"same\")(upsample)\n",
    "    concat = Concatenate(axis=3)([residual, upconv])\n",
    "\n",
    "    conv1 = Conv2D(filters, (3, 3), padding='same')(concat)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters, (3, 3), padding='same')(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "\n",
    "    return conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_fcn1(input_layer, filters):\n",
    "   filters = int(filters)\n",
    "   upsample = UpSampling2D()(input_layer)\n",
    "   upconv = Conv2D(filters, kernel_size=(2, 2), padding=\"same\")(upsample)\n",
    "\n",
    "   conv1 = Conv2D(filters, (3, 3), padding='same')(upconv)\n",
    "   conv1 = BatchNormalization()(conv1)\n",
    "   conv1 = Activation('relu')(conv1)\n",
    "   return conv1\n",
    "\n",
    "\n",
    "#def up_fcn2(input_layer, filters):\n",
    "#    filters = int(filters)\n",
    "#    upsample = Conv2DTranspose(filters, (3, 3), padding='same',strides=(2,2))(input_layer)\n",
    "#    return upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "\n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "\n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "\n",
    "    weights = K.variable(weights)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use Tensorflow to write our own dice_coefficient metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1e-5\n",
    "\n",
    "    y_true = tf.round(tf.reshape(y_true, [-1]))\n",
    "    y_pred = tf.round(tf.reshape(y_pred, [-1]))\n",
    "\n",
    "    isct = tf.reduce_sum(y_true * y_pred)\n",
    "\n",
    "    return 2 * isct / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unet_model():\n",
    "    # Make a custom U-nets implementation.\n",
    "    filters = 32\n",
    "    #input_layer = Input(shape=[256, 256, 1])\n",
    "    input_layer = Input(shape=[64, 64, 1])\n",
    "    layers = [input_layer]\n",
    "    residuals = []\n",
    "\n",
    "    # Down 1, 128\n",
    "    d1, res1 = down(input_layer, filters)\n",
    "    residuals.append(res1)\n",
    "\n",
    "    filters *= 2\n",
    "\n",
    "    # Down 2, 64\n",
    "    d2, res2 = down(d1, filters)\n",
    "    residuals.append(res2)\n",
    "\n",
    "    filters *= 2\n",
    "\n",
    "    # Down 3, 32\n",
    "    d3, res3 = down(d2, filters)\n",
    "    residuals.append(res3)\n",
    "\n",
    "    filters *= 2\n",
    "\n",
    "    # Down 4, 16\n",
    "    d4, res4 = down(d3, filters)\n",
    "    residuals.append(res4)\n",
    "\n",
    "    filters *= 2\n",
    "\n",
    "    # Down 5, 8\n",
    "    d5 = down(d4, filters, pool=False)\n",
    "\n",
    "    # Up 1, 16\n",
    "    up1 = up(d5, residual=residuals[-1], filters=filters / 2)\n",
    "\n",
    "    filters /= 2\n",
    "\n",
    "    # Up 2,  32\n",
    "    up2 = up(up1, residual=residuals[-2], filters=filters / 2)\n",
    "\n",
    "    filters /= 2\n",
    "\n",
    "    # Up 3, 64\n",
    "    up3 = up(up2, residual=residuals[-3], filters=filters / 2)\n",
    "\n",
    "    filters /= 2\n",
    "\n",
    "    # Up 4, 128\n",
    "    up4 = up(up3, residual=residuals[-4], filters=filters / 2)\n",
    "\n",
    "    fc1 = Conv2D(filters=32, kernel_size=(1, 1), activation=\"relu\")(up4)\n",
    "\n",
    "    out = Conv2D(filters=2, kernel_size=(1, 1), activation=\"softmax\")(fc1)\n",
    "\n",
    "    model = Model(input_layer, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fcn_models():\n",
    "    # Make a custom U-nets implementation.\n",
    "    filters = 32\n",
    "    #input_layer = Input(shape=[256, 256, 1])\n",
    "    input_layer = Input(shape=[64, 64, 1])\n",
    "    layers = [input_layer]\n",
    "    residuals = []\n",
    "\n",
    "    # Down 1, 128\n",
    "    d1, res1 = down(input_layer, filters)\n",
    "    residuals.append(res1)\n",
    "\n",
    "    filters *= 2\n",
    "\n",
    "    # Down 2, 64\n",
    "    d2, res2 = down(d1, filters)\n",
    "    residuals.append(res2)\n",
    "\n",
    "    filters *= 2\n",
    "\n",
    "    # Down 3, 32\n",
    "    d3, res3 = down(d2, filters)\n",
    "    residuals.append(res3)\n",
    "\n",
    "    filters *= 2\n",
    "\n",
    "    # Down 4, 16\n",
    "    d4, res4 = down(d3, filters)\n",
    "    residuals.append(res4)\n",
    "\n",
    "    filters *= 2\n",
    "\n",
    "    # Down 5, 8\n",
    "    d5 = down(d4, filters, pool=False)\n",
    "    \n",
    "    # Up 1, 16\n",
    "    up1 = up_fcn1(d5, filters=filters / 2)\n",
    "    up1 = Add()([up1, residuals[-1]])\n",
    "        \n",
    "    filters /= 2\n",
    "\n",
    "    # Up 2,  32\n",
    "    up2 = up_fcn1(up1, filters=filters / 2)\n",
    "    up2 = Add()([up2, residuals[-2]])\n",
    "    \n",
    "    models = {}\n",
    "\n",
    "    fcn16s = UpSampling2D(size=(16, 16))(d5)\n",
    "    fcn16s_out = Conv2D(filters=2, kernel_size=(1, 1), activation=\"softmax\")(fcn16s)\n",
    "    models['fcn16s'] = Model(input_layer, fcn16s_out)\n",
    "    \n",
    "    fcn8s = UpSampling2D(size=(8, 8))(up1)\n",
    "    fcn8s_out = Conv2D(filters=2, kernel_size=(1, 1), activation=\"softmax\")(fcn8s)\n",
    "    models['fcn8s'] = Model(input_layer, fcn8s_out)\n",
    "    \n",
    "    fcn4s = UpSampling2D(size=(4, 4))(up2)\n",
    "    fcn4s_out = Conv2D(filters=2, kernel_size=(1, 1), activation=\"softmax\")(fcn4s)\n",
    "    models['fcn4s'] = Model(input_layer, fcn4s_out)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.compile(optimizer=Adam(1e-4), loss=weighted_categorical_crossentropy([1.0, 7.0]), metrics=[dice_coef])\n",
    "    hist = model.fit(X_train, Y_train, epochs=10, batch_size=128, verbose=1, validation_data=(X_val, Y_val))\n",
    "    return hist\n",
    "\n",
    "# Saving model\n",
    "def save_model(model, model_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name + '.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(model_name + '.h5')\n",
    "    print('Saved model to disk, {}'.format(model_name))\n",
    "    \n",
    "#Loading model\n",
    "def load_model(model_name):\n",
    "    json_file = open(model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(model_name + '.h5')\n",
    "    print('Loaded model from disk, {}'.format(model_name))\n",
    "    return loaded_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = make_unet_model()\n",
    "models = make_fcn_models()\n",
    "models['unet'] = unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "for name, model in models.items():\n",
    "    print(name + ' model')\n",
    "    model.summary()\n",
    "    history[name] = train(model)\n",
    "    save_model(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "def plot_loss(history, title=''):\n",
    "    loss_train = history.history['loss']\n",
    "    loss_val = history.history['val_loss']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(loss_train, 'b', label='train loss')\n",
    "    ax.plot(loss_val, 'r', label='val loss')\n",
    "    ax.legend()\n",
    "    ax.set_title('loss - ' + title)\n",
    "    plt.show()\n",
    "\n",
    "for key, value in history.items():\n",
    "    plot_loss(value, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    predictions[name] = model.predict(X_test, verbose=0)\n",
    "    \n",
    "# Plotting the result images\n",
    "def show_results(images, names):\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "\n",
    "    for idx, img in enumerate(images):\n",
    "        ax = fig.add_subplot(1, 3, idx + 1)    \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(names[idx])\n",
    "    plt.show()\n",
    "    \n",
    "idx = int(500)\n",
    "img_in =  np.squeeze(X_test[idx,])\n",
    "img_ans = np.argmax(Y_test[idx,], axis=2)\n",
    "for key, value in predictions.items():\n",
    "    print(key)\n",
    "    img_pred = np.argmax(value[idx,], axis=2)\n",
    "    show_results([img_in, img_ans, img_pred],\n",
    "               ['input', 'label', 'predict'])\n",
    "\n",
    "# Calculating Dice coefficient\n",
    "def calc_dice_coeff(labels, predicts):\n",
    "    counts = predicts.shape[0]\n",
    "    dice_score = np.zeros(shape = [counts,1])\n",
    "\n",
    "    for i in range(counts):\n",
    "        img_train_ans = np.argmax(labels[i,], axis=2)\n",
    "        img_train_out = np.argmax(predicts[i,], axis=2)\n",
    "\n",
    "        isct = np.sum(np.sum(img_train_ans * img_train_out))\n",
    "        dice_score[i,0] = 2*isct / (np.sum(np.sum(img_train_ans)) + np.sum(np.sum(img_train_out)) + 1e-5)\n",
    "\n",
    "    avg_dice_score = np.sum(dice_score[:,0]) / np.count_nonzero(dice_score[:,0]) * 100\n",
    "#     print(\"Dice coefficient: %.2f %%\" % avg_dice_score)\n",
    "    return avg_dice_score\n",
    "\n",
    "def calc_score(model):\n",
    "    val_score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    test_score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "    return val_score, test_score\n",
    "\n",
    "for name, model in models.items():\n",
    "    test_score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    test_dice_coeff = calc_dice_coeff(Y_test, predictions[name])\n",
    "    print('{} model'.format(name))\n",
    "    print('    test score : {}'.format(name, test_score))\n",
    "    print(\"    Dice coefficient for test set: %.2f %%\" % test_dice_coeff)\n",
    "\n",
    "# val_dice_coeff = calc_dice_coeff(Y_val, prediction_val)\n",
    "# print(\"Dice coefficient for val set: %.2f %%\" % val_dice_coeff)\n",
    "\n",
    "# unet_model.compile(optimizer=Adam(1e-4), loss=weighted_categorical_crossentropy([1.0, 7.0]), metrics=[dice_coef])\n",
    "\n",
    "\n",
    "# prediction_train = unet_model.predict(X_train[0:1000, ], verbose=0)\n",
    "# prediction_val = unet_model.predict(X_val[0:1000, ], verbose=0)\n",
    "# prediction_test = unet_model.predict(X_test, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
